{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d52d8b",
   "metadata": {},
   "source": [
    "# Lab 08 - Model Prediction Speed of Deep Neural Networks\n",
    "\n",
    "During this lab, we will continue exploring the prediction speed of models. This time we will focus\n",
    "on deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a481b603",
   "metadata": {},
   "source": [
    "## 1. Model Serving\n",
    "\n",
    "*Below, there is a proposed scenario for you to implement. However, if you have another idea in\n",
    "mind, feel free to use a different dataset or choose another task of comparable complexity.*\n",
    "\n",
    "We will focus on an image classification task. The building and training a model is not the main\n",
    "focus of this lab, so we want this step to be as straightforward as possible. Therefore, we will\n",
    "focus on preparing a model that can distinguish between ants and bees using the \"Hymenoptera\"\n",
    "dataset - thus, you can get the inspiration and a starting point from the following PyTorch tutorial\n",
    "[Transfer Learning for Computer Vision Tutorial](\n",
    "https://docs.pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#transfer-learning-for-computer-vision-tutorial).\n",
    "On that page, you will find also a link to download the dataset.\n",
    "\n",
    "Follow the tutorial to obtain a trained model for the task, save it appropriately and serve\n",
    "it using a model serving-framework of your choice.\n",
    "\n",
    "There are many options available, for example:\n",
    "- implement a custom Flask or FastAPI application that serves the model over a REST API\n",
    "- use MLServe - [MLServer](https://github.com/SeldonIO/MLServer) - see \"Serving a custom model\"\n",
    "- use LitServe - [LitServe](https://github.com/Lightning-AI/LitServe) - you can test it also locally\n",
    "- use Ray Serve - [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) - see \"PyTorch\" tab\n",
    "- use NVIDIA Triton Inference Server:\n",
    "    - https://github.com/triton-inference-server/server\n",
    "    - https://github.com/triton-inference-server/tutorials\n",
    "    \n",
    "In short, you should serve the model and be able to send requests with image representations (there\n",
    "are different options, such as raw images, preprocessed tensors, etc., depending on the design of\n",
    "your serving solution). The server should respond with the predicted class (ant or bee) and/or the\n",
    "associated probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192e6b5",
   "metadata": {},
   "source": [
    "## 2. Measure Inference Speed\n",
    "\n",
    "We are interested in the performance of the serving setup. Similar to the previous lab, we can use a\n",
    "general-purpose tool for load testing and benchmarking web services, for example,\n",
    "[Locust](https://locust.io/).\n",
    "\n",
    "Measure the inference speed, including the latency and the number of requests per second that your\n",
    "serving solution can handle. Try to estimate what is the limit of your setup with respect to the\n",
    "RPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4aeecd",
   "metadata": {},
   "source": [
    "## 3. Experiment with Models and Serving Options\n",
    "\n",
    "Experiment with different model architectures. In particular, try to prepare several models of\n",
    "significantly different sizes and compare latency and inference RPS (requests per second) that you\n",
    "can achieve.\n",
    "\n",
    "Experiment also with different serving options, such as parallelization, model quantization and protocols\n",
    "(REST vs. gRPC), etc. Try to draw conclusions from the results. Can you observe any difference in the\n",
    "inference speed? Does batching influence the results? If possible, provide plots to visualize your\n",
    "findings. You can obtain raw data from the load-testing tool `Locust` for further analysis \\-\n",
    "https://docs.locust.io/en/stable/retrieving-stats.html#retrieve-test-statistics-in-csv-format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
