{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d52d8b",
   "metadata": {},
   "source": [
    "# Lab 08 - Model Prediction Speed of Deep Neural Networks\n",
    "\n",
    "During this lab, we will continue exploring the prediction speed of models. This time we will focus\n",
    "on deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a481b603",
   "metadata": {},
   "source": [
    "## 1. Model Serving\n",
    "\n",
    "*Below, there is a proposed scenario for you to implement. However, if you have another idea in\n",
    "mind, feel free to use a different dataset or choose another task of comparable complexity.*\n",
    "\n",
    "We will focus on an image classification task. The building and training a model is not the main\n",
    "focus of this lab, so we want this step to be as straightforward as possible. We will focus on\n",
    "preparing a model that can distinguish between ants and bees using the \"Hymenoptera\" dataset - you\n",
    "can get the inspiration and a starting point (or even follow the larger part of the code) from the\n",
    "PyTorch tutorial [Transfer Learning for Computer Vision Tutorial](\n",
    "https://docs.pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#transfer-learning-for-computer-vision-tutorial).\n",
    "On that page, you will find also a link to download the dataset.\n",
    "\n",
    "Follow the tutorial to obtain a trained model for the task, fine-tune it, save it appropriately and\n",
    "serve it using a model serving-framework of your choice.\n",
    "\n",
    "There are many options available, for example:\n",
    "- use NVIDIA Triton Inference Server:\n",
    "    - https://github.com/triton-inference-server/server\n",
    "    - https://github.com/triton-inference-server/tutorials\n",
    "- use Ray Serve - [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) - see \"PyTorch\" tab\n",
    "- use LitServe - [LitServe](https://github.com/Lightning-AI/LitServe) - you can test it also locally\n",
    "- use MLServe - [MLServer](https://github.com/SeldonIO/MLServer) - see \"Serving a custom model\"\n",
    "- implement a custom Flask or FastAPI application that serves the model over a REST API\n",
    "    \n",
    "In short, you should serve the model and be able to send requests with image representations (there\n",
    "are different options, such as raw images, preprocessed tensors, etc., depending on the design of\n",
    "your serving solution). The server should respond with the predicted class (ant or bee) and/or the\n",
    "associated probabilities.\n",
    "\n",
    "Remark 1: Ensure that your serving solution can accept batched requests (you may enforce the upper\n",
    "limit on the batch size) and that each image can also be of a different resolution. Depending on the \n",
    "solution you choose, you may need to use features such as dynamic input shapes or implement \n",
    "custom preprocessing logic to handle varying image sizes. Some interesting resources that may help:\n",
    "- https://docs.pytorch.org/docs/2.9/export.html#expressing-dynamism\n",
    "\n",
    "Remark 2: You may need to study some documentation or tutorials to make the chosen serving solution\n",
    "work (sometimes) more efficiently, e.g., you might use static computation graph instead of the eager\n",
    "mode. Some interesting resources that may help:\n",
    "- https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html\n",
    "- https://onnxruntime.ai/docs/\n",
    "- https://onnx.ai/\n",
    "- https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tutorials/Quick_Deploy/PyTorch/README.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192e6b5",
   "metadata": {},
   "source": [
    "## 2. Measure Inference Speed\n",
    "\n",
    "We are interested in the performance of the serving setup. Similar to the previous lab, we can use a\n",
    "general-purpose tool for load testing and benchmarking web services, for example,\n",
    "[Locust](https://locust.io/).\n",
    "\n",
    "Measure the inference speed, including the latency and the number of requests per second that your\n",
    "serving solution can handle. Try to estimate what is the limit of your setup with respect to the\n",
    "RPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4aeecd",
   "metadata": {},
   "source": [
    "## 3. Experiment with Models and Serving Options\n",
    "\n",
    "Experiment with different model architectures. In particular, try to use other pre-trained models\n",
    "available - see https://docs.pytorch.org/vision/main/models.html#classification for some examples. \n",
    "In particular, try to choose models with different sizes and computational requirements.\n",
    "\n",
    "Experiment also with different serving options. Try to draw conclusions from the results. Can you\n",
    "observe any difference in the inference speed? Does batching influence the results? If possible,\n",
    "provide plots to visualize your findings. You can obtain raw data from the load-testing tool\n",
    "`Locust` for further analysis \\-\n",
    "https://docs.locust.io/en/stable/retrieving-stats.html#retrieve-test-statistics-in-csv-format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
