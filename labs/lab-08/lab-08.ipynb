{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d52d8b",
   "metadata": {},
   "source": [
    "# Lab 08 - Model Prediction Speed of Deep Neural Networks\n",
    "\n",
    "During this lab, we will continue exploring the prediction speed of models. This time we will focus\n",
    "on deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a481b603",
   "metadata": {},
   "source": [
    "## 1. Model Serving\n",
    "\n",
    "We will focus on an object detection task. \n",
    "\n",
    "Select a model from a deep learning library (e.g., TensorFlow, PyTorch, etc.) that is suitable for\n",
    "object detection tasks. You could start with pre-trained weights and fine-tune the model on a\n",
    "dataset of your choice. Finally, save the model and serve it using NVIDIA Triton Inference Server.\n",
    "\n",
    "If you prefer, you may use a different dataset of similar size or choose another task of comparable\n",
    "complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192e6b5",
   "metadata": {},
   "source": [
    "## 2. Measure Inference Speed\n",
    "\n",
    "We are interested in the performance of the serving setup. Similar to the previous lab, we can use a\n",
    "general-purpose tool for load testing and benchmarking web services. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4aeecd",
   "metadata": {},
   "source": [
    "## 3. Experiment with Models and Serving Options\n",
    "\n",
    "Experiment with different model architectures. In particular, try to prepare several models of\n",
    "significantly different sizes and compare latency and inference RPS (requests per second) that you\n",
    "can achieve.\n",
    "\n",
    "Experiment also with different serving options, such as parallelization, model quantization and protocols\n",
    "(REST vs. gRPC), etc. Try to draw conclusions from the results. Can you observe any difference in the\n",
    "inference speed? Does batching influence the results? If possible, provide plots to visualize your\n",
    "findings. You can obtain raw data from the load-testing tool `Locust` for further analysis \\-\n",
    "https://docs.locust.io/en/stable/retrieving-stats.html#retrieve-test-statistics-in-csv-format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
