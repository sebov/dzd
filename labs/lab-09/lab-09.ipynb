{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b0385f",
   "metadata": {},
   "source": [
    "# Lab 09 - Data Quality Monitoring\n",
    "\n",
    "During this lab we will explore techniques for monitoring data quality in machine learning systems.\n",
    "\n",
    "By \"data quality\" we refer to the characteristics of data with respect to its completeness,\n",
    "correctness and reliability. Poor data quality can lead to models of low performance and unreliable\n",
    "predictions.\n",
    "\n",
    "The examples of data quality issues include:\n",
    "- missing values\n",
    "- incorrect or inconsistent data entries\n",
    "- presence of outliers\n",
    "- impossible values\n",
    "- contradictory data\n",
    "- inconsistent formatting\n",
    "- wrong data types\n",
    "- duplicate records\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d2f99",
   "metadata": {},
   "source": [
    "## 1. Data Quality Validation\n",
    "\n",
    "Data validation is a process of checking if the data conforms to predefined rules and constraints.\n",
    "It is used to ensure that the data is accurate, complete, and reliable before it is used for\n",
    "analysis or modeling.\n",
    "\n",
    "Here, we focus mainly on automated data validation techniques that can be integrated into data\n",
    "pipelines. This is somewhat different from the data validation that is performed at runtime in\n",
    "applications to validate user inputs, web form data, etc.\n",
    "\n",
    "There are multiple open source (some of them are a part of larger commercial systems) libraries\n",
    "available for data validation/data quality analysis in Python. Some of the popular ones include:\n",
    "\n",
    "- Great Expectations - https://github.com/great-expectations/great_expectations\n",
    "- Pandera - https://pandera.readthedocs.io\n",
    "- Pointblank - https://posit-dev.github.io/pointblank/\n",
    "- Soda Core - https://github.com/sodadata/soda-\n",
    "- Deequ - https://github.com/awslabs/deequ\n",
    "\n",
    "Your task is to implement the following scenario:\n",
    "\n",
    "- Use the [NYC Yellow Taxi Trip\n",
    "  Data](https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data) from Kaggle, which is a\n",
    "  subset of the original dataset provided by the [NYC Taxi & Limousine Commission\n",
    "  (TLC)](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "- Familiarize yourself with the dataset - gain a basic understanding of the domain it represents to\n",
    "  identify potential data quality issues.\n",
    "- For the sake of the exercise, we will introduce some errors/disturbances into the dataset. This\n",
    "  allows us to practice data quality validation techniques while being already aware of the errors.\n",
    "  For example:\n",
    "    - replace some values randomly with NaN\n",
    "    - for a categorical attribute (e.g., `store_and_fwd_flag` or `payment_type`) replace some items\n",
    "      with new values that should be considered incorrect\n",
    "    - change some values to be outside of the expected range, e.g., `trip_distance` is negative,\n",
    "      `latitude/longitude` are outside of New York City\n",
    "    - change some integer-valued attributes to have fractional parts, e.g., `passenger_count`\n",
    "    - introduce some errors that are correct with respect to the domain of particular attributes but\n",
    "      invalid when considering relationships between attributes, e.g., start date is later than\n",
    "      end date or `total_amount` is not equal to the sum of other amounts\n",
    "    - for \"cash\" value of `payment_type` set `tip_amount` greater than 0 for some items (normaly,\n",
    "      tip amount is reported as `0` for cash payments)\n",
    "    - add 3-4 more errors/disturbances of your choice\n",
    "- Use one of the libraries mentioned above (you need to choose one) to model the data validation\n",
    "  rules and detect the introduced errors/disturbances. Some of the libraries also allow you to\n",
    "  obtain the severity level of the detected errors based on defined thresholds. Here, you are in a\n",
    "  privileged position as you already know which errors were introduced into the dataset. However,\n",
    "  try to leverage the exploratory data features of the library to detect the errors/disturbances\n",
    "  before implementing the rules. \n",
    "  \n",
    "  Implement rules that cover the errors/disturbances that you have introduced into the dataset. You\n",
    "  should also implement the rules that check basic properties of the dataset, for example:\n",
    "\n",
    "    - ensure the dataset contains the appropriate number of columns\n",
    "    - ensure the types of the attributes are correct\n",
    "    - etc.\n",
    "\n",
    "- Investigate the results. Depending on the library, you should be able to generate a\n",
    "  machine-readable report and/or a human-readable report. A human-readable report can be reviewed by\n",
    "  a person responsible for data quality, while a machine-readable report can be used to implement\n",
    "  automated pipelines that can detect errors/disturbances in the data and take appropriate actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6169078",
   "metadata": {},
   "source": [
    "## 2. Data Monitoring\n",
    "\n",
    "Data monitoring involves tracking data quality over time to detect any issues that may arise. This\n",
    "is important because data quality issues can result from various factors, such as changes in data\n",
    "sources, data collection processes, or data processing pipelines. By monitoring data quality,\n",
    "systems can identify problems early and address them before they impact the operation of machine\n",
    "learning models.\n",
    "\n",
    "Some features of data monitoring may overlap with data validation. The boundary between the\n",
    "approaches can sometimes be blurry. For example, you can implement data monitoring procedures using\n",
    "data validation tools within data ingestion pipelines or maintain data quality within scheduled\n",
    "workflows.\n",
    "\n",
    "There are multiple tools available for data monitoring. Just to name a few:\n",
    "\n",
    "- Evidently - https://docs.evidentlyai.com/quickstart_ml\n",
    "- WhyLabs-oss + WhyLogs\n",
    "    - https://github.com/whylabs/whylabs-oss\n",
    "    - https://docs.whylabs.ai/docs/\n",
    "- Stream DaQ - https://bilpapster.github.io/stream-DaQ\n",
    "\n",
    "Your task is to implement one of the below scenarios:\n",
    "\n",
    "- Prepare a simple REST service (with a database - e.g., SQLite, PostgreSQL, etc.) that allows users\n",
    "  to upload batches of data. You can assume the batches will be relatively small (no more than ten\n",
    "  thousand rows). You may use the same dataset as in the previous section. The service should\n",
    "  support uploading data batches in file format, e.g., CSV, Parquet, etc. (it is up to you to choose\n",
    "  the format). You don't need to implement all the validation rules from the previous section\n",
    "  - select as many as possible without compromising the performance of the service. Return an\n",
    "  appropriate HTTP status code and, in case of an error, provide error information. Additionally,\n",
    "  provide an endpoint that reports the number of rows in the database.\n",
    "- Implement data stream monitoring using Stream DaQ on the following dataset:\n",
    "  https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption or any\n",
    "  other dataset of similar size. Get familiar with the features of the monitoring tool. You may need\n",
    "  to preprocess the dataset, e.g., to create a proper timestamp column. Be creative and implement\n",
    "  various quality checks. For example:\n",
    "  - Are there as many readings as expected within a given time window?\n",
    "  - Are there gaps in the data?\n",
    "  - Are the voltage readings within the expected bounds?\n",
    "  - Do the current intensity readings exceed the maximum allowed value for the electric fuse (you\n",
    "    can decide what this value should be)?\n",
    "  - Is the sum of energy consumption from sub-metering devices less than the total energy\n",
    "    consumption? Consider that the units differ: sub-metering devices use \"watt-hour of active\n",
    "    energy\", whereas the global active power device reports \"minute-averaged active power in\n",
    "    kilowatts\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
