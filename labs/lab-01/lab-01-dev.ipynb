{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61d579f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lab 01 - Techniques for Handling Large Datasets in Classification\n",
    "\n",
    "In this scenario we will explore several techniques for handling large datasets in classification tasks. We will compare the performance of different approaches, including:\n",
    "\n",
    "- processing the full dataset in parts\n",
    "- sampling techniques\n",
    "- summarization techniques\n",
    "- quantization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f6e36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. The Dataset\n",
    "\n",
    "During this lab, we will use the [NYC Yellow Taxi Trip Data](https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data) from Kaggle.\n",
    "In fact, this is a subset of the much larger and continuously updated dataset available from the [NYC Taxi & Limousine Commission (TLC)](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). The original dataset is evolving over time, with respect to its schema, therefore to avoid issues, we will use a preprocessed version from a Kaggle user.\n",
    "\n",
    "Get familiar with the dataset page, read the description, check available columns, as well as their types and meaning. Download all available CSV files - use any means you prefer (Kaggle API, kagglehub package, direct download using browser, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77295438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385adb5",
   "metadata": {},
   "source": [
    "## 2. Investigate the Dataset\n",
    "\n",
    "Get familiar with the dataset. Check the sizeof the files, the number of rows and columns, the actual data types, etc. You can use a tool of your choice (pandas, dask, pyspark, pyarrow, etc.) - the one you can install and run locally or an online tool it supports large files. Is it possible to load the entire dataset in memory? Even if it is possible in your case, try to think about scenarios where there is not enough memory available. Consequently, you should design your solutions so that they do not require loading the entire dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18573d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea349aac",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Dataset\n",
    "\n",
    "Clean and preprocess the dataset, e.g., handle missing values, engineer new features, perform discretization, etc. You may approach this task in an iterative manner, i.e., you may implement a first version and then back to it later, if needed.\n",
    "\n",
    "### Classification task\n",
    "\n",
    "There is no predefined classification task for this dataset. You should define your own. Prepare your solution introducing some problem parameters so that they can be easily changed in the future. For example:\n",
    "\n",
    "- You may want to predict whether the tip amount will be above a certain threshold `LARGE_TIP_THRESHOLD` (e.g., above 20% of the total amount or total amount with tip excluded) - investigate the distribution of the target variable with respect to different values of `LARGE_TIP_THRESHOLD`, plot it, and choose a reasonable value. A hint: the tip amount is reported only for cashless payments, so you may want to first filter the dataset to include only such records.\n",
    "- You may want to predict whether the total amount will be above a certain threshold `HIGH_TOTAL_AMOUNT_THRESHOLD` - what is a reasonable value for this threshold?\n",
    "- etc.\n",
    "\n",
    "What evaluation approach will you use? How will you split the dataset into training and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33cb2d4",
   "metadata": {},
   "source": [
    "## 4. Implement Different Techniques for Handling Large Datasets\n",
    "\n",
    "### Classification Model\n",
    "\n",
    "Implement a simple Naive Bayes classifier on your own that can be applied to large datasets - it should be able to process the dataset in parts, e.g., by processing one column or batch of rows at a time. Alternatively, you can use different model from available libraries, as long as it can be applied to large datasets. Be creative!\n",
    "\n",
    "### Techniques for Handling Large Datasets\n",
    "\n",
    "You can incorporate the following techniques for handling large datasets directly in your solution, or you can split the process into two steps - first, preprocess the data and save it (use `parquet` format), then use the saved file to train and evaluate the model. Prepare your solution so that you can easily change the parameters of the techniques you use and compare the results, computation time and the size of the processed data used for training the model.\n",
    "\n",
    "- Process the full dataset in parts - process the dataset in parts, e.g., by processing one column at a time for Naive Bayes (compute the necessary distributions for each column) or by processing batches of rows and creating an ensemble of models.\n",
    "- Sampling techniques - sample a subset of the dataset, e.g., using random sampling, stratified sampling, - introduce a parameter `FRAC` that defines the fraction of the dataset to be sampled.\n",
    "- Summarization techniques - apply clustering/gridding/binning to group similar instances and aggregate the values within each cluster/bin. E.g., you can use location-related features (pickup and dropoff coordinates) to create bins and aggregate (you can take the mean for numerical features and the majority value \"*mode*\" for categorical features) the rides which have pickup and dropoff locations within the same bin. \n",
    "- Quantization techniques - reduce the number of distinct values for numerical features, e.g., by applying rounding, flooring, ceiling, or other techniques. Store values with reduced precision - it can reduce I/O time and memory usage and consequently speed up the computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be315234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer, OrdinalEncoder\n",
    "import datetime\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = pathlib.Path(\".\") / \"..\" / \"..\" / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f70ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_DIR / \"taxi_cash.parquet\")\n",
    "total_gb = df.memory_usage(deep = True).sum() / 10**9\n",
    "print(f\"Memory usage: {total_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d427af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"RateCodeID\", \"RatecodeID\"], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df = df.sort_values(by=\"tpep_pickup_datetime\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = df.sample(frac=0.05).index.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd820f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_TIP_THRESHOLD = 0.17\n",
    "\n",
    "data = df.loc[sample_index].copy()\n",
    "tip_amount = data[\"tip_amount\"]\n",
    "data.drop(columns=[\"tip_amount\"], inplace=True)\n",
    "target = tip_amount >= LARGE_TIP_THRESHOLD * (data[\"total_amount\"] - tip_amount)\n",
    "time = data[\"tpep_pickup_datetime\"].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b163a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = [\n",
    "    col\n",
    "    for col in data.select_dtypes(include=np.float64).columns\n",
    "    if data[col].nunique() > 5\n",
    "]\n",
    "float_cols_1 = [\n",
    "    \"trip_distance\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"fare_amount\",\n",
    "    \"extra\",\n",
    "    \"tolls_amount\",\n",
    "]\n",
    "float_cols_2 = [\n",
    "    \"total_amount\",\n",
    "]\n",
    "\n",
    "datetime_cols = [col for col in data.select_dtypes(include=np.datetime64).columns]\n",
    "object_cols = [col for col in data.select_dtypes(include=object).columns]\n",
    "\n",
    "transformer_nb = make_column_transformer(\n",
    "    # (\n",
    "    #     KBinsDiscretizer(\n",
    "    #         n_bins=100, encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n",
    "    #     ),\n",
    "    #     float_cols,\n",
    "    # ),\n",
    "    (\n",
    "        KBinsDiscretizer(\n",
    "            n_bins=5, encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n",
    "        ),\n",
    "        float_cols_1,\n",
    "    ),\n",
    "    (\n",
    "        KBinsDiscretizer(\n",
    "            n_bins=100,\n",
    "            encode=\"ordinal\",\n",
    "            strategy=\"quantile\",\n",
    "            # n_bins=100, encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n",
    "        ),\n",
    "        float_cols_2,\n",
    "    ),\n",
    "    (\n",
    "        FunctionTransformer(lambda x: x.apply(lambda col: col.dt.isocalendar().week)),\n",
    "        datetime_cols,\n",
    "    ),\n",
    "    (\n",
    "        OrdinalEncoder(),\n",
    "        object_cols,\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "encoder_nb = OrdinalEncoder()\n",
    "\n",
    "transformer_nb_2 = make_column_transformer(\n",
    "    (\n",
    "        KBinsDiscretizer(\n",
    "            n_bins=100,\n",
    "            encode=\"ordinal\",\n",
    "            strategy=\"quantile\",\n",
    "            # n_bins=100, encode=\"ordinal\", quantile_method=\"averaged_inverted_cdf\"\n",
    "        ),\n",
    "        float_cols_2,\n",
    "    ),\n",
    "    (\n",
    "        FunctionTransformer(lambda x: x.apply(lambda col: col.dt.isocalendar().week)),\n",
    "        datetime_cols,\n",
    "    ),\n",
    "    (\n",
    "        OrdinalEncoder(),\n",
    "        object_cols,\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "encoder_nb_2 = OrdinalEncoder()\n",
    "\n",
    "\n",
    "\n",
    "transformer_tree = make_column_transformer(\n",
    "    (\n",
    "        FunctionTransformer(lambda x: x.apply(lambda col: col.dt.isocalendar().week)),\n",
    "        datetime_cols,\n",
    "    ),\n",
    "    (\n",
    "        OrdinalEncoder(),\n",
    "        object_cols,\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "\n",
    "data_nb = encoder_nb.fit_transform(transformer_nb.fit_transform(data))\n",
    "\n",
    "data_nb_2 = encoder_nb_2.fit_transform(\n",
    "    transformer_nb_2.fit_transform(\n",
    "        data.drop(\n",
    "            columns=[\n",
    "                \"trip_distance\",\n",
    "                \"pickup_longitude\",\n",
    "                \"pickup_latitude\",\n",
    "                \"dropoff_longitude\",\n",
    "                \"dropoff_latitude\",\n",
    "                \"fare_amount\",\n",
    "                \"extra\",\n",
    "                \"tolls_amount\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "data_tree = transformer_tree.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\n",
    "            columns=[\n",
    "                \"trip_distance\",\n",
    "                \"pickup_longitude\",\n",
    "                \"pickup_latitude\",\n",
    "                \"dropoff_longitude\",\n",
    "                \"dropoff_latitude\",\n",
    "                \"fare_amount\",\n",
    "                \"extra\",\n",
    "                \"tolls_amount\",\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc641d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29aba1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14424f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(cl, X, y):\n",
    "    mask = (time < datetime.date(2016, 3, 1)).to_numpy()\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "\n",
    "    mask = time > datetime.date(2016, 3, 15)\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "\n",
    "\n",
    "\n",
    "    cl.fit(X_train, y_train)\n",
    "    print(balanced_accuracy_score(y_test, cl.predict(X_test)))\n",
    "    return cl\n",
    "\n",
    "\n",
    "cl = assess(DecisionTreeClassifier(min_samples_leaf=5000), data_tree, target)\n",
    "cl2 = assess(CategoricalNB(), data_nb, target)\n",
    "cl3 = assess(CategoricalNB(), data_nb_2, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ffef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf4116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 20))\n",
    "plot_tree(\n",
    "    cl,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=12,\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
