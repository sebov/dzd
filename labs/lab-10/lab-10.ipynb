{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81591287",
   "metadata": {},
   "source": [
    "# Lab 10 - Data-Quality-Driven Machine Learning Systems\n",
    "\n",
    "During this lab we will experiment with data quality driven machine learning systems. The main focus\n",
    "will be on data quality metrics and how they can be used to improve/fix the performance of machine\n",
    "learning models in the presence of data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78f507",
   "metadata": {},
   "source": [
    "## 1. Machine Learning System\n",
    "\n",
    "Your task is to create a classifier ensemble that can predict the target variable based on the\n",
    "features provided in a dataset. For this lab, we will use the Human Activity Recognition Using\n",
    "Smartphones dataset -\n",
    "https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones. It is a\n",
    "classification problem where we predict the activity performed by users wearing a smarphone (Samsung\n",
    "Galaxy S II) on the waist. The possible activities are LAYING, STANDING, SITTING, WALKING,\n",
    "WALKING_UPSTAIRS, WALKING_DOWNSTAIRS. The embedded accelerometer and gyroscope were used to capture\n",
    "sensor signals. Overall, the dataset contains 561 input features, either raw or aggregated. It also\n",
    "includes the target variable that we want to predict and a subject identifier indicating which\n",
    "participant carried out the experiment. The dataset is also available in CSV format on Kaggle -\n",
    "https://www.kaggle.com/datasets/uciml/human-activity-recognition-with-smartphones/data.\n",
    "\n",
    "\n",
    "Use the training subset to prepare an ensemble of models that operate on diverse subsets of\n",
    "features. You can use any machine learning model (e.g. a decision tree) of your choice and a\n",
    "majority voting scheme to aggregate the predictions. The choice of feature subsets should ensure\n",
    "robustness of the ensemble. For example, you might create models that utilize:\n",
    "\n",
    "- only raw features\n",
    "- only raw features and derivatives\n",
    "- only aggregated features\n",
    "- only aggregated features of particular type (mean, standard deviation, etc.)\n",
    "- only raw features from accelerometer\n",
    "- only raw and aggregated features from accelerometer\n",
    "- only raw features from gyroscope\n",
    "- only raw and aggregated features from gyroscope\n",
    "- feature selection methods to select a subset of features\n",
    "- let machine learning algorithm to select features automatically\n",
    "- etc.\n",
    "\n",
    "You should be able to define dozens of different models easily. However, it is important to track\n",
    "which features are utilized by each component of the ensemble. Use a data validation\n",
    "framework/library (e.g., the one you familiarized yourself with during previous labs) to define data\n",
    "quality rules. You can start with basic rules, such as checking for missing values or values out of\n",
    "bounds, and configure the library to obtain machine-readable error reports. \n",
    "\n",
    "During inference, if any issues are detected with a given feature, you should disable/mute the\n",
    "models that utilize this feature. For example, if a problem is detected with any of the raw\n",
    "accelerometer features, aggregate predictions only from models that do not use this feature.\n",
    "\n",
    "You should also anticipate cases where all features used by your ensemble have quality issues. In\n",
    "these scenarios, prepare a fallback procedure - for example, return the most common activity.\n",
    "\n",
    "Perform experiments to evaluate the performance of your ensemble under certain data quality issues.\n",
    "You are required to simulate sensor failures, such as missing values or introduce out-of-bounds\n",
    "values. Compare the ensemble's performance against a baseline model trained on the same dataset and\n",
    "evaluated under the same data quality issues, but without any data quality handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41872471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4674ed3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2*. Anomaly Detection\n",
    "\n",
    "Anomaly detection is the task of identifying rare events or outliers in a dataset. However, it\n",
    "differs from data quality validation in that its focus is not on ensuring data correctness, but\n",
    "rather on identifying unexpected or rare patterns. Although distinct, both anomaly detection and\n",
    "data quality issues can influence the performance of machine learning models, but for different\n",
    "reasons. Data quality issues usually concern the validity of data (e.g., missing or incorrect\n",
    "values), while anomaly detection focuses on unusual patterns and outliers.\n",
    "\n",
    "Sometimes, distinguishing between the two can be challenging. For example, consider a dust or\n",
    "fine-particle sensor. If a sensor in the kitchen registers a maximum value while food is being\n",
    "prepared on a frying pan, this could be considered an anomaly, yet it might also be a theoretically\n",
    "correct reading (if the sensor has an upper limit on its readings range). On the other hand, if the\n",
    "sensor stops providing readings due to a battery outage, this can be considered a data quality issue\n",
    "rather than an anomaly in the dust sensor data stream (but depending on the domain context, it can\n",
    "be interprted as an anomaly as well).\n",
    "\n",
    "*There is also another aspect to consider - the presence of data drift. Data drift refers to the\n",
    "process by which a data distribution changes over time. It can be caused by various factors, such as\n",
    "changes in the environment, modifications of the data collection process, or shifts in the\n",
    "underlying data itself. Data drift can have a significant impact on the performance of machine\n",
    "learning models, as it may cause them to become less accurate over time if they were trained on a\n",
    "different data distribution. For now, however, we will treat data drift as a separate topic that is\n",
    "out of the scope of this lab.*\n",
    "\n",
    "Tere are several open-source libraries that can be used for anomaly detection. Sometime these are\n",
    "libraries specialized in anomaly detection for specific domains or data types. Just to name a few:\n",
    "\n",
    "- Alibi Detect - https://docs.seldon.ai/alibi-detect\n",
    "- PyOD - https://pyod.readthedocs.io/en/latest/\n",
    "- Anomalib - https://anomalib.readthedocs.io/en/latest/\n",
    "\n",
    "Familiarize yourself with the above libraries. Choose one of them and experiment with the dataset\n",
    "used in the previous section. Simulate/introduce some anomalies into the dataset and try to detect\n",
    "them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda27f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
