{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d52d8b",
   "metadata": {},
   "source": [
    "# Lab 07 - Model Prediction Speed\n",
    "\n",
    "During this lab we will explore the prediction speed of different models and model serving\n",
    "frameworks. This is important for understanding how model architecture, deployment environment,\n",
    "and other factors affect real-time inference performance and scalability.\n",
    "\n",
    "There are multiple approaches to serving machine learning models in production environments. We can\n",
    "imagine the following scenarios (the may overlap):\n",
    "\n",
    "- batch prediction: where we predict on a large number of samples at once, e.g., offline predictions\n",
    "  done on a daily basis\n",
    "- real-time prediction: where we need to make predictions as soon as new data arrives, e.g., online\n",
    "  predictions for user recommendations or fraud detection systems\n",
    "- streaming prediction: where we predict on data streams, e.g., financial market predictions or IoT\n",
    "  sensor data\n",
    "- low-latency prediction: where we need to minimize the time between receiving input and producing\n",
    "  output\n",
    "- edge deployment: where models are deployed on devices with limited computational resources\n",
    "- embedded model: where models are integrated into applications without a separate serving layer,\n",
    "  e.g., a mobile app that uses a pre-trained model for image classification\n",
    "\n",
    "The above approaches can be addressed using a custom-built solutions or by leveraging existing\n",
    "libraries, services or frameworks. \n",
    "\n",
    "For now, we will focus mainly on **real-time** and **low-latency predictions**. The training time is\n",
    "out of scope for this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1e217",
   "metadata": {},
   "source": [
    "## 1. Model Serving\n",
    "\n",
    "There are multiple ways to serve machine learning models. Some popular options include:\n",
    "\n",
    "- building a custom REST API using Flask or FastAPI frameworks, etc.\n",
    "- using model serving frameworks specific to a particular ML ecosystem, e.g., TensorFlow Serving,\n",
    "  TorchServe, etc.\n",
    "- general-purpose or multi-framework model serving platforms, e.g., MLflow, BentoML, MLServer,\n",
    "  Seldon Core, KServe, NVIDIA Triton Inference Server, ONNX Runtime, OpenVINO, Ray Serve, etc. Some\n",
    "  of these use other serving runtimes under the hood.\n",
    "- frameworks supporting advanced inference logic and ML workflows, such as inference graphs,\n",
    "  ensembles, or workflow pipelines, e.g., Seldon Tempo SDK, Kubeflow Pipelines, KServe, etc.\n",
    "\n",
    "Sometimes, the above options may overlap.\n",
    "\n",
    "There exist some standards and API protocols designed to facilitate model serving, e.g.:\n",
    "- KServe V1 Protocol based on the TensorFlow Serving API:\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane/v1-protocol\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane\n",
    "- Open Inference Protocol (KServe V2 Protocol) - endorsed also by NVIDIA Triton Inference Server,\n",
    "  TensorFlow Serving and TorchServe\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane/v2-protocol\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane\n",
    "- API protocols based on the OpenAI API specification for large language models (LLM) inference\n",
    "- etc.\n",
    "\n",
    "Your first task during this lab is to train an XGBoost or SKLearn model on a dataset of your choice\n",
    "and serve it using at least two different model serving options - MLServer and a custom FastAPI\n",
    "endpoint. If you prefer, you may choose different model or serving frameworks instead of the\n",
    "suggested ones.\n",
    "\n",
    "Then, try requesting predictions from both serving options. In case of MLServer, try to use both\n",
    "the REST and gRPC endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2a06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656df62",
   "metadata": {},
   "source": [
    "## 2. Measure Inference Performance\n",
    "\n",
    "We are interested in the performance of our serving setups. Due to the fact that they are deployed\n",
    "as services either HTTP REST or gRPC endpoints, we can use general-purpose tools for load testing\n",
    "and benchmarking web services. Some popular options include:\n",
    "- Locust\n",
    "- k6\n",
    "- Apache JMeter\n",
    "- Vegeta\n",
    "- etc.\n",
    "\n",
    "Your second task during this lab is to measure the inference performance of your serving setups from\n",
    "previous exercise using Locust, k6, or any other tool of your choice.\n",
    "\n",
    "We are mainly interested in the following metrics:\n",
    "- Latency (response time) - average, median, p95, p99\n",
    "- Throughput (requests per second)\n",
    "- Error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c851d",
   "metadata": {},
   "source": [
    "## 3. Experiment with Models and Serving Options\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
