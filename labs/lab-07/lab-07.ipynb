{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d52d8b",
   "metadata": {},
   "source": [
    "# Lab 07 - Model Prediction Speed\n",
    "\n",
    "During this lab we will explore the prediction speed of different models and model serving\n",
    "frameworks. This is important for understanding how model architecture, deployment environment,\n",
    "and other factors affect real-time inference performance and scalability.\n",
    "\n",
    "There are multiple approaches to serving machine learning models in production environments. We can\n",
    "imagine the following scenarios (the may overlap):\n",
    "\n",
    "- batch prediction: where we predict on a large number of samples at once, e.g., offline predictions\n",
    "  done on a daily basis\n",
    "- real-time prediction: where we need to make predictions as soon as new data arrives, e.g., online\n",
    "  predictions for user recommendations or fraud detection systems\n",
    "- streaming prediction: where we predict on data streams, e.g., financial market predictions or IoT\n",
    "  sensor data\n",
    "- low-latency prediction: where we need to minimize the time between receiving input and producing\n",
    "  output\n",
    "- edge deployment: where models are deployed on devices with limited computational resources\n",
    "- embedded model: where models are integrated into applications without a separate serving layer,\n",
    "  e.g., a mobile app that uses a pre-trained model for image classification\n",
    "\n",
    "The above approaches can be addressed using a custom-built solutions or by leveraging existing\n",
    "libraries, services or frameworks. \n",
    "\n",
    "For now, we will focus mainly on **real-time** and **low-latency predictions**. The training time is\n",
    "out of scope for this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1e217",
   "metadata": {},
   "source": [
    "## 1. Model Serving\n",
    "\n",
    "There are multiple ways to serve machine learning models. Some popular options include:\n",
    "\n",
    "- in-process model serving within an application, e.g., loading a model directly in a Python code\n",
    "  and using it for predictions\n",
    "- building a custom REST API using Flask or FastAPI frameworks, etc.\n",
    "- using model serving frameworks specific to a particular ML ecosystem, e.g., TensorFlow Serving,\n",
    "  TorchServe, etc.\n",
    "- general-purpose or multi-framework model serving platforms, e.g., MLflow, BentoML, MLServer,\n",
    "  Seldon Core, KServe, NVIDIA Triton Inference Server, ONNX Runtime, OpenVINO, Ray Serve, etc. Some\n",
    "  of these use other serving runtimes under the hood.\n",
    "- frameworks supporting advanced inference logic and ML workflows, such as inference graphs,\n",
    "  ensembles, or workflow pipelines, e.g., Seldon Tempo SDK, Kubeflow Pipelines, KServe, etc.\n",
    "\n",
    "Sometimes, the above options may overlap.\n",
    "\n",
    "There exist some standards and API protocols designed to facilitate model serving, e.g.:\n",
    "- KServe V1 Protocol based on the TensorFlow Serving API:\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane/v1-protocol\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane\n",
    "- Open Inference Protocol (KServe V2 Protocol) - endorsed also by NVIDIA Triton Inference Server,\n",
    "  TensorFlow Serving and TorchServe\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane/v2-protocol\n",
    "    - https://kserve.github.io/website/docs/concepts/architecture/data-plane\n",
    "- API protocols based on the OpenAI API specification for large language models (LLM) inference\n",
    "- etc.\n",
    "\n",
    "Your first task during this lab is to train an XGBoost or SKLearn model on a dataset of your choice.\n",
    "If train and test splits are not provided, please create them.\n",
    "\n",
    "Propositions for a dataset to use:\n",
    "- https://www.openml.org/search?type=data&sort=runs&id=150&status=active\n",
    "\n",
    "  ```python\n",
    "  from sklearn.datasets import fetch_openml\n",
    "  bunch = fetch_openml(\"Covertype\", return_X_y=False, version=3)\n",
    "  print(bunch.data.shape, bunch.target.shape)\n",
    "  ```\n",
    "- https://www.kaggle.com/competitions/ieee-fraud-detection/data\n",
    "- https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/data\n",
    "\n",
    "Then, store the model in a suitable format and serve it using at least two different model serving\n",
    "options \\- MLServer and a custom FastAPI endpoint. If you prefer, you may choose different dataset\n",
    "(however, please use a dataset of similar size and complexity), model, or serving frameworks instead\n",
    "of the suggested ones.\n",
    "\n",
    "Remark: As of writing this lab, there is an open issue in MLServer\n",
    "https://github.com/SeldonIO/MLServer/issues/2286. Therefore, if you encounter any issues related to\n",
    "`uvloop`, try downgrading this package.\n",
    "\n",
    "Then, try requesting predictions from both serving options. In case of MLServer, try to use both\n",
    "the REST and gRPC endpoints. Try batched (if supported) and single-sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656df62",
   "metadata": {},
   "source": [
    "## 2. Measure Inference Performance\n",
    "\n",
    "We are interested in the performance of our serving setups. Due to the fact that they are deployed\n",
    "as services either HTTP REST or gRPC endpoints, we can use general-purpose tools for load testing\n",
    "and benchmarking web services. Some popular options include:\n",
    "- `Locust` (https://locust.io/)\n",
    "- `k6` (https://k6.io/)\n",
    "- `Apache JMeter` (https://jmeter.apache.org/)\n",
    "- `Vegeta` (https://github.com/tsenart/vegeta)\n",
    "- etc.\n",
    "\n",
    "Your second task during this lab is to measure the inference speed of the serving setup from\n",
    "previous exercise using `Locust` or any other tool of your choice. Use data test subset for\n",
    "generating requests.\n",
    "\n",
    "This may not be the best possible benchmarking setup, as it runs load generation on the same machine\n",
    "as the model server, but it should be sufficient for learning purposes. Get familiar with the both\n",
    "tools - the serving framework and the load testing tool you chose. There are many caveats to\n",
    "properly benchmarking served models - they are out of scope for this lab, but be aware of some of\n",
    "them:\n",
    "- https://docs.locust.io/en/stable/increasing-request-rate.html#concurrency\n",
    "- https://docs.locust.io/en/stable/increasing-request-rate.html#load-generation-performance\n",
    "- https://docs.locust.io/en/stable/increasing-request-rate.html#actual-issues-with-the-system-under-test\n",
    "\n",
    "We are mainly interested in the following metrics:\n",
    "- Latency (response time) - e.g., average, median, p95, p99\n",
    "- Throughput (requests per second)\n",
    "- Error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c851d",
   "metadata": {},
   "source": [
    "## 3. Experiment with Models and Serving Options\n",
    "\n",
    "The last task during this lab is to experiment with different model hyperparameters, architectures,\n",
    "and sizes. Try settings that affect the size and complexity of XGBoost models (if you decided to use\n",
    "it), e.g., depth of decision trees, number of estimators, etc. We want to see if these changes\n",
    "impact inference speed.\n",
    "\n",
    "Experiment with different serving options: number of workers/replicas, single-sample vs. batch\n",
    "prediction (when using batches of different sizes, take this into account when comparing results,\n",
    "e.g., normalize latency or throughput per sample), protocol type (REST vs gRPC), etc. Try to\n",
    "formulate some conclusions based on your observations: if/how specific factors affect inference\n",
    "speed metrics. Is a larger model slower? Can you explain why? What is the maximum speed (requests\n",
    "per second) you can achieve on your hardware? Does batching affect latency and throughput? If\n",
    "possible, provide some plots to visualize your findings - you can obtain data from `Locust` for\n",
    "further analysis\n",
    "https://docs.locust.io/en/stable/retrieving-stats.html#retrieve-test-statistics-in-csv-format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
