{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d52d8b",
   "metadata": {},
   "source": [
    "# Lab 08 - Model Prediction Speed of Deep Neural Networks\n",
    "\n",
    "During this lab, we will continue exploring the prediction speed of models. This time we will focus\n",
    "on deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a481b603",
   "metadata": {},
   "source": [
    "## 1. Model Serving\n",
    "\n",
    "We will focus on an object detection task. \n",
    "\n",
    "The first task is to select a model from a popular deep learning library, you may use pre-trained\n",
    "weights to start with, but you should be able to fine-tune the model on a dataset of your choice.\n",
    "Then, save the model and serve it using NVIDIA Triton Inference Server.\n",
    "\n",
    "\n",
    "If you prefer, you may use a different dataset of similar size or choose another task of comparable\n",
    "complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192e6b5",
   "metadata": {},
   "source": [
    "## 2. Measure Inference Speed\n",
    "\n",
    "We are interested in the performance of the serving setup. Similar to the previous lab, we can use a\n",
    "general-purpose tool for load testing and benchmarking web services. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4aeecd",
   "metadata": {},
   "source": [
    "## 3. Experiment with Models and Serving Options\n",
    "\n",
    "Experiment with different model architectures. In particular, try to prepare several models of\n",
    "significantly different sizes and compare latency and inference RPS (requests per second) that you\n",
    "can achieve.\n",
    "\n",
    "Experiment also with different serving options, such as parallelization, model quantization and protocols\n",
    "(REST vs. gRPC), etc. Try to draw conclusions from the results. Can you observe any difference in the\n",
    "inference speed? Does batching influence the results? If possible, provide plots to visualize your\n",
    "findings. You can obtain raw data from the load-testing tool `Locust` for further analysis \\-\n",
    "https://docs.locust.io/en/stable/retrieving-stats.html#retrieve-test-statistics-in-csv-format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659d4136",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[Errno 2] No such file or directory: 'test/assets/encode_jpeg/grace_hopper_517x606.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_bounding_boxes\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_pil_image\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m img = \u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest/assets/encode_jpeg/grace_hopper_517x606.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Step 1: Initialize model with the best available weights\u001b[39;00m\n\u001b[32m      9\u001b[39m weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/synced/backup/workspace/dzd/.venv/lib/python3.13/site-packages/torchvision/io/image.py:321\u001b[39m, in \u001b[36mdecode_image\u001b[39m\u001b[34m(input, mode, apply_exif_orientation)\u001b[39m\n\u001b[32m    319\u001b[39m     _log_api_usage_once(decode_image)\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    323\u001b[39m     mode = ImageReadMode[mode.upper()]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/synced/backup/workspace/dzd/.venv/lib/python3.13/site-packages/torchvision/io/image.py:64\u001b[39m, in \u001b[36mread_file\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_tracing():\n\u001b[32m     63\u001b[39m     _log_api_usage_once(read_file)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/synced/backup/workspace/dzd/.venv/lib/python3.13/site-packages/torch/_ops.py:1255\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [Errno 2] No such file or directory: 'test/assets/encode_jpeg/grace_hopper_517x606.jpg'"
     ]
    }
   ],
   "source": [
    "from torchvision.io.image import decode_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = decode_image(\"test/assets/encode_jpeg/grace_hopper_517x606.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c26f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
