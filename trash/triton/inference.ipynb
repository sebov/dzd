{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model = torchvision.models.resnet50(weights='IMAGENET1K_V2')\n",
    "model.eval()\n",
    "traced_model = torch.jit.trace(model, torch.randn(1, 3, 224, 224))\n",
    "torch.jit.save(traced_model, \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8dceb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def rn50_preprocess(img_path=\"img1.jpg\"):\n",
    "    img = Image.open(img_path)\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    return preprocess(img).numpy()\n",
    "\n",
    "transformed_img = rn50_preprocess()\n",
    "transformed_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61e29e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "514f0952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3, 224, 224)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_imgs = np.stack([transformed_img, transformed_img, transformed_img, transformed_img, transformed_img, transformed_img, transformed_img]) \n",
    "batch_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac176329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = httpclient.InferInput(\"input__0\", batch_imgs.shape, datatype=\"FP32\")\n",
    "inputs.set_data_from_numpy(batch_imgs, binary_data=True)\n",
    "\n",
    "outputs = httpclient.InferRequestedOutput(\n",
    "    \"output__0\", binary_data=True, class_count=1000\n",
    ")\n",
    "results = client.infer(model_name=\"resnet50\", inputs=[inputs], outputs=[outputs])\n",
    "inference_output = results.as_numpy(\"output__0\")\n",
    "inference_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b63d44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
